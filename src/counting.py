"""Script to generate word-frequency values from Large text file, using multiprocessing techniques.
   Word-frequency pairs are stored as one-per-line <Word, Frequency> CSVs in file 'out_file'.
   Usage: python counting.py [filename]. """

import sys, re, os
from multiprocessing import Pool, cpu_count
from itertools import izip_longest
import trie

def read_in_chunks (fname, chunk_size=1024):
    """Generate file contents, chunk-by-chunk, as specified by chunk_size."""
    with open(fname) as file_object:
        while True:
            data = file_object.read(chunk_size)
            if not data:
                break
            while data[-1].isalnum():
                data = data + file_object.read(1)
            yield data

def build_trie(piece):
    """Return word trie, built from file chunk."""
    my_trie = trie.Trie()
    words = [word.strip().lower() for word in re.findall (r'\w+', piece)]
                #Parse only words, ignoring punctuation and spaces

    for word in words:
        if my_trie.lookup(word) == None:
            my_trie.add(word, 1) # if word not contained in trie, add it
        else:
        	#print "found word ", word, " again"
            my_trie.add(word,my_trie.lookup(word) + 1) #Otherwise increment its frequency value
    return my_trie


def recurse_wrapper(results,  out_file):
    """For query search, recurses all tries generated by subprocesses."""
    if len(results) == 1:
    	return recurse_trie(results[0],[], '', out_file)
    else:
        trie, rest = results[0], results[1:]
        recurse_trie(trie, rest,'',  out_file)
        recurse_wrapper(rest, out_file)

def lookup_rest(rest, string):
    "Return frequency count of 'string' accumulated across all subtries."""
    count_to_return = 0
    for subtrie in rest:
        val = subtrie.lookup(string)
        if val is not None:
            count_to_return = count_to_return + val
            subtrie.add(string, None) #Set value to None, so future lookups don't double-count it
    return count_to_return

def recurse_trie(trie, rest, string, out_file):
    """Recurse trie and write <word,frequency> pairs to out_file.
       param: 'trie' refers to current trie.
       param: 'rest' refers to all other tries generated by subprocesses."""

    if trie.value is not None:
        #lookup string in every other trie FORWARD.
        out_val = trie.value + lookup_rest(rest, string)
        out_file.write(string + "," + str(out_val) + '\n')

    for char, node in trie.root.iteritems(): #Build up string and continue recursion
        recurse_trie(node, rest, string+char, out_file)

if __name__ == "__main__":
    if len(sys.argv) is not 2:
        sys.exit('Wrong Argument Length. Usage: python counting.py [filename]')

    filename = sys.argv[1]
    CHUNK_SIZE = 1024*1024*10 #Seemingly optimal, could be changed according to file specs

    N_WORKERS = cpu_count() - 1 #Get number of cores that can be utilized
    my_results = []

    with open(filename) as file_in:
        if N_WORKERS > 0:
            pool = Pool(N_WORKERS) #Spawn subprocesses for each chunk
            #Get result tries of each subprocess in my_results
            my_results = pool.map(build_trie, read_in_chunks(filename, CHUNK_SIZE))

    out_file = open('/Users/talwai/dev/rapcountr/out_files/' + filename.split('/')[1], 'w')

    #Recurse across results and print out <word, frequency> pairs
    recurse_wrapper(my_results, out_file)

    out_file.close()
